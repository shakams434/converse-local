# AI LLM Assistant - Asistente Bancario Offline

Una aplicaciÃ³n mÃ³vil hÃ­brida construida con React + Capacitor que funciona como un "contenedor de modelos" offline para ejecutar modelos LLM localmente.

## ğŸ¯ CaracterÃ­sticas Principales

- **GestiÃ³n de Modelos**: Importa y gestiona archivos .gguf en el dispositivo
- **Motor LLM Local**: Interfaz para conectar con llama.cpp (actualmente con mock)
- **Chat Inteligente**: Interface de chat con streaming de respuestas en tiempo real
- **ConfiguraciÃ³n Avanzada**: System prompts personalizables y parÃ¡metros del motor
- **100% Offline**: No requiere conexiÃ³n a internet para funcionar
- **MÃ³vil Nativo**: Funciona en Android e iOS como app nativa

## ğŸ—ï¸ Arquitectura

### Stack TecnolÃ³gico
- **Frontend**: React 18 + TypeScript + Vite
- **UI**: Tailwind CSS + shadcn/ui + Tema oscuro optimizado
- **Estado**: Zustand con persistencia
- **MÃ³vil**: Capacitor para iOS/Android
- **NavegaciÃ³n**: Tabs nativas con React Router

### Estructura del Proyecto
```
src/
â”œâ”€â”€ components/          # Pantallas principales
â”‚   â”œâ”€â”€ NavigationTabs.tsx
â”‚   â”œâ”€â”€ ModelManagerScreen.tsx
â”‚   â”œâ”€â”€ ChatScreen.tsx
â”‚   â””â”€â”€ SettingsScreen.tsx
â”œâ”€â”€ store/              # Estado global (Zustand)
â”‚   â””â”€â”€ llmStore.ts
â”œâ”€â”€ modules/local-llm/  # Interface del Motor LLM
â”‚   â”œâ”€â”€ types.ts        # Interfaces TypeScript
â”‚   â”œâ”€â”€ mock.ts         # ImplementaciÃ³n mock
â”‚   â””â”€â”€ index.ts        # Factory y exports
â”œâ”€â”€ utils/              # Utilidades
â”‚   â””â”€â”€ fileUtils.ts    # GestiÃ³n de archivos
â””â”€â”€ pages/              # Rutas principales
    â”œâ”€â”€ Index.tsx
    â””â”€â”€ NotFound.tsx
```

## ğŸ“± Pantallas

### 1. Gestor de Modelos (`ModelManagerScreen`)
- **Importar Modelos**: Selecciona archivos .gguf del dispositivo
- **ConfiguraciÃ³n**: nCtx (contexto) y nThreads (hilos de procesamiento)
- **Estado del Motor**: Idle â†’ Loading â†’ Ready â†’ Error
- **Control**: Cargar/Recargar modelo en memoria

### 2. Chat (`ChatScreen`)
- **Interface de Chat**: Burbujas usuario/asistente con timestamps
- **Streaming**: Respuestas en tiempo real token por token
- **Controles**: Stop generaciÃ³n, Reset conversaciÃ³n
- **Estado Visual**: Indicadores de estado del motor

### 3. ConfiguraciÃ³n (`SettingsScreen`)
- **System Prompt**: Instrucciones que definen el comportamiento del AI
- **ConfiguraciÃ³n**: Forzar respuestas en espaÃ±ol
- **Aplicar al Motor**: Sincronizar configuraciÃ³n con el motor LLM

## ğŸ”§ Native Module Interface

El proyecto incluye una interfaz completa para el Native Module que se conectarÃ¡ con llama.cpp:

```typescript
interface LocalLLM {
  loadModel(path: string, opts?: ModelLoadOptions): Promise<boolean>;
  setSystemPrompt(prompt: string): Promise<void>;
  generate(input: string, opts?: GenerationOptions): Promise<void>;
  stop(): Promise<void>;
  reset(): Promise<void>;
  getStatus(): Promise<EngineStatus>;
  addTokenListener(callback: TokenListener): () => void;
}
```

### Mock Implementation
- Simula carga de modelos con delays realistas
- Streaming de tokens con timing variable
- Respuestas bancarias predefinidas para testing
- Control completo de start/stop/reset

## ğŸš€ CÃ³mo Ejecutar

### Desarrollo Web
```bash
# Instalar dependencias
npm install

# Ejecutar en modo desarrollo
npm run dev
```

### Mobile Deployment con Capacitor

Esta app estÃ¡ optimizada para dispositivos mÃ³viles usando Capacitor. Sigue estos pasos para ejecutar en Android/iOS:

#### Prerrequisitos
- Node.js y npm instalados
- Para Android: Android Studio con SDK
- Para iOS: macOS con Xcode instalado

#### Pasos de Build y Deploy

1. **Clonar y Setup**
```bash
git clone <your-repo>
cd <project-folder>
npm install
```

2. **Agregar Plataformas MÃ³viles** (si no estÃ¡n agregadas)
```bash
npx cap add android
npx cap add ios
```

3. **Scripts Disponibles para MÃ³vil**
```bash
# Build y sync todas las plataformas
npm run cap:sync

# Build y abrir Android Studio
npm run cap:android

# Build y abrir Xcode
npm run cap:ios
```

4. **Deploy a Dispositivo**
- **Android**: Usar Android Studio para ejecutar en emulador o dispositivo conectado
- **iOS**: Usar Xcode para ejecutar en simulador o dispositivo conectado

#### CaracterÃ­sticas EspecÃ­ficas para MÃ³vil

- **Native File Picker**: Usa `@capawesome/capacitor-file-picker` para seleccionar archivos .gguf
- **File System**: Los modelos se almacenan en `Documents/Models/` usando `@capacitor/filesystem`
- **UI Mobile-First**: NavegaciÃ³n con tabs inferiores, controles tÃ¡ctiles optimizados (â‰¥44px), layout de una columna
- **Capacidad Offline**: Sin dependencias de red, funciona 100% localmente

#### Troubleshooting

- **Android**: Asegurar que USB debugging estÃ© habilitado y el dispositivo sea reconocido
- **iOS**: Requiere cuenta de Apple Developer para deploy en dispositivo
- **File Access**: Otorgar permisos de almacenamiento cuando se solicite
- **Performance**: Usar nCtx recomendado (1024-2048) y nThreads (2-4) para dispositivos mÃ³viles

### Requisitos MÃ³viles
- **Android**: Android Studio + SDK
- **iOS**: macOS + Xcode + iOS SDK

## ğŸ§ª Testing del Mock

El mock estÃ¡ activo por defecto y simula:

1. **Importar modelo**: Archivos .gguf simulados con tamaÃ±os realistas
2. **Cargar en motor**: Delay de 2-3 segundos, luego estado "ready"
3. **Chat**: Respuestas bancarias en espaÃ±ol con streaming
4. **System prompt**: AplicaciÃ³n inmediata al motor mock
5. **Stop/Reset**: Control completo de la generaciÃ³n

## ğŸ”„ PrÃ³ximos Pasos

### Reemplazar Mock por Native Module

1. **Android (JNI Bridge)**:
   - Crear mÃ³dulo nativo en `android/app/src/main/java/`
   - Integrar llama.cpp como biblioteca nativa
   - Implementar callbacks para streaming

2. **iOS (Swift/ObjC Bridge)**:
   - Crear mÃ³dulo nativo en `ios/App/App/`
   - Integrar llama.cpp como framework
   - Implementar delegates para streaming

3. **DetecciÃ³n de Plataforma**:
   - Modificar `src/modules/local-llm/index.ts`
   - Detectar si es web (mock) o nativo (real)

### Estructura Native Modules

```
native/
â”œâ”€â”€ android/
â”‚   â”œâ”€â”€ LocalLLMModule.java
â”‚   â”œâ”€â”€ LlamaEngine.cpp
â”‚   â””â”€â”€ CMakeLists.txt
â”œâ”€â”€ ios/
â”‚   â”œâ”€â”€ LocalLLMModule.swift
â”‚   â”œâ”€â”€ LlamaEngine.mm
â”‚   â””â”€â”€ LlamaEngine-Bridging-Header.h
â””â”€â”€ shared/
    â””â”€â”€ llama.cpp/  # Submodule
```

## ğŸ“Š Estado Actual

- âœ… **UI Completa**: Todas las pantallas implementadas
- âœ… **Mock Funcional**: Testing completo sin motor real
- âœ… **Estado Persistente**: ConfiguraciÃ³n guardada en AsyncStorage
- âœ… **Capacitor Setup**: Listo para compilaciÃ³n mÃ³vil
- âœ… **Streaming UI**: Interface responsive con streaming de tokens
- â³ **Native Module**: Pendiente reemplazar mock por llama.cpp

## ğŸ¨ Design System

- **Colores**: Tema oscuro con acentos azul/verde
- **Gradientes**: Sutiles para botones y cards
- **TipografÃ­a**: Inter/Roboto optimizada para mÃ³vil
- **Animaciones**: Transiciones suaves y typing indicators
- **Responsive**: Optimizado para todas las pantallas

## ğŸ’¡ CaracterÃ­sticas TÃ©cnicas

- **Offline First**: Sin dependencias de red
- **Performance**: Optimizado para dispositivos con â‰¤4GB RAM
- **Persistence**: Estado guardado automÃ¡ticamente
- **Error Handling**: Manejo robusto de errores y estados
- **TypeScript**: Tipado estricto en todo el proyecto
- **Mobile UX**: Interface tÃ¡ctil optimizada

---

**Estado del Proyecto**: âœ… **Listo para testing y desarrollo del Native Module**

El mock permite probar toda la funcionalidad mientras se desarrolla la integraciÃ³n real con llama.cpp.